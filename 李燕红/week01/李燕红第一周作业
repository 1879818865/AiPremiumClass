{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建ndarray数组\n",
    "#使⽤array() 函数创建数组时，参数必须是⽅括号括起来的列表\n",
    "a1 = [1,2,3,4,5,6,7,8,9,10]\n",
    "b1 = np.array(a1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 或者\n",
    "a1 = np.array([1,2,3,4,5,6,7,8,9,10], float)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建多维数组\n",
    "a2 = np.array([(1,2,3), (4,5,6), (7,8,9)])\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建特殊数组\n",
    "# NumPy提供了⼀些使⽤占位符创建数组的函数。\n",
    "# zeros\n",
    "# ones\n",
    "a3 = np.zeros((2,3), dtype = int)\n",
    "a3\n",
    "\n",
    "b3 = np.ones((3,3))\n",
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建等差数列\n",
    "#arange\n",
    "a4 = np.arange(1, 6, 0.5, dtype = float)\n",
    "a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建单位矩阵\n",
    "#one-hot编码\n",
    "#eye\n",
    "a5 = np.eye((5), dtype = int)\n",
    "a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79612219, 0.86953865, 0.16951733, 0.32050465, 0.91141225])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#⽣成指定⻓度，在 [0,1) 之间平均分布的随机数组\n",
    "#模型运算参数初始值\n",
    "a6=np.random.random(5)\n",
    "a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07742091, -0.13617307,  0.06158202,  0.01997501, -0.00717237])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#⽣成指定⻓度，符合正态分布的随机数组，指定其均值为 0，标准差为 0.1\n",
    "#模型运算参数初始值\n",
    "a7,b7 = 0, 0.1\n",
    "np.random.normal(a7, b7, 5)\n",
    "\n",
    "a7 = np.random.normal(0, 0.1, 5)  \n",
    "a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# NumPy 数组的访问\n",
    "# 支持切片操作。可以切⽚每⼀个维度，索引每⼀个维度。\n",
    "a8 = np.array([(1,2), (3,4), (5,6)])\n",
    "#print(a8)\n",
    "#print(a8[:,0])\n",
    "#print(a8[1:])\n",
    "#print(a8[: , :1])\n",
    "print(a8[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n",
      "5 6\n",
      "5 6\n"
     ]
    }
   ],
   "source": [
    "# NumPy 数组的遍历\n",
    "a9 = np.array([(1,2), (3,4), (5,6), (5,6)])\n",
    "for i,j in a9:\n",
    "#    print(i*j)\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndim: 2\n",
      "shape: (3, 2)\n",
      "size 6\n",
      "dtype int64\n"
     ]
    }
   ],
   "source": [
    "#NumPy 数组的常⽤属性\n",
    "#ndarray.ndim : 数组的维度（数组轴的个数），等于秩 \n",
    "#ndarray.shape : 数组的⼤⼩。为⼀个表⽰数组在每个维度上⼤⼩的整数元组。例如⼆维 数组中，表⽰数组的’ ⾏数’ 和’ 列数” \n",
    "#ndarray.size : 数组元素的总个数，等于 shape 属性中元组元素的乘积 \n",
    "#ndarray.dtype : 表⽰数组中元素类型的对象\n",
    "a = np.array([(1,2), (3,4), (5,6)])\n",
    "print(\"ndim:\", a.ndim)\n",
    "print(\"shape:\", a.shape)\n",
    "print(\"size\", a.size)\n",
    "print(\"dtype\", a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#检测数值是否在数组中\n",
    "a = np.array([(1,2), (3,4), (5,6)])\n",
    "print(8 in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [2]\n",
      "  [3]]\n",
      "\n",
      " [[4]\n",
      "  [5]\n",
      "  [6]]\n",
      "\n",
      " [[7]\n",
      "  [8]\n",
      "  [9]]]\n"
     ]
    }
   ],
   "source": [
    "#reshape : 数组的重排列，例如将⼀个3维数组转变1维（元素数⼀定要保持不变）\n",
    "c2 = np.arange(1,10)\n",
    "#print(c2)\n",
    "#print(c2.shape)\n",
    "\n",
    "c3 = c2.reshape(3,3,1)  # 维度大小乘积 == 元素个数\n",
    "print(c3)\n",
    "#print(c3.shape)  # 高维矩阵，每个维度都有含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tanspose ：转置 (可以直接.T）\n",
    "a = np.array([(1,2), (3,4), (5,6)])\n",
    "a = a.T\n",
    "print(a)\n",
    "\n",
    "a.transpose()\n",
    "\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatten : 把多维数组转换为⼀维数组，注意每个元组的⻓度是相同的\n",
    "a = np.array([(1,2), (3,4), (5,6)])\n",
    "a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#newaxis : 增加维度\n",
    "a = np.array([(1,2), (3,4), (5,6)])\n",
    "a = a[:,:,np.newaxis]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2.]\n",
      " [0. 2.]]\n",
      "[[2. 0.]\n",
      " [2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#NumPy 数组的数学操作\n",
    "a = np.ones((2,2))\n",
    "b = np.array([(-1,1),(-1,1)])\n",
    "print(a+b)\n",
    "print(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum: 188\n",
      "prod: 5455650816\n",
      "mean: 26.857142857142858\n",
      "var: 120.6938775510204\n",
      "std: 10.986076531274502\n",
      "max: 48\n",
      "min: 12\n",
      "argmax: 6\n",
      "argmin: 2\n",
      "ceil: [16 31 12 28 22 31 48]\n",
      "floor: [16 31 12 28 22 31 48]\n",
      "rint: [16. 31. 12. 28. 22. 31. 48.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12, 16, 22, 28, 31, 31, 48])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求和、求积、平均数，⽅差，标准差，最⼤值，最⼩值\n",
    "a = np.array([16,31,12,28,22,31,48])\n",
    "a.sum()\n",
    "print(\"sum:\",a.sum())\n",
    "print(\"prod:\",a.prod())\n",
    "print(\"mean:\",a.mean())\n",
    "print(\"var:\", a.var())\n",
    "print(\"std:\", a.std())\n",
    "print(\"max:\", a.max())\n",
    "print(\"min:\", a.min())\n",
    "#最⼤与最⼩值对应的索引值：argmax,argmin:\n",
    "#取元素值上限，下限，四舍五⼊：ceil, floor, rint\n",
    "print(\"argmax:\", a.argmax())\n",
    "print(\"argmin:\", a.argmin())\n",
    "print(\"ceil:\", np.ceil(a))\n",
    "print(\"floor:\", np.floor(a))\n",
    "print(\"rint:\", np.rint(a))\n",
    "#排序：sort\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵 1:\n",
      "[[1. 2.]\n",
      " [5. 6.]]\n",
      "矩阵 2:\n",
      "[[3. 4.]\n",
      " [7. 8.]]\n",
      "使用 np.dot 得到的矩阵乘法结果:\n",
      "[[17. 20.]\n",
      " [57. 68.]]\n",
      "使用 @ 运算符得到的矩阵乘法结果:\n",
      "[[17. 20.]\n",
      " [57. 68.]]\n",
      "1.0 * 3.0 = 3.0\n",
      "2.0 * 7.0 = 14.0\n",
      "结果矩阵[1,1]:17.0\n",
      "\n",
      "1.0 * 4.0 = 4.0\n",
      "2.0 * 8.0 = 16.0\n",
      "结果矩阵[1,2]:20.0\n",
      "\n",
      "5.0 * 3.0 = 15.0\n",
      "6.0 * 7.0 = 42.0\n",
      "结果矩阵[2,1]:57.0\n",
      "\n",
      "5.0 * 4.0 = 20.0\n",
      "6.0 * 8.0 = 48.0\n",
      "结果矩阵[2,2]:68.0\n",
      "\n",
      "手动推演结果:\n",
      "[[17. 20.]\n",
      " [57. 68.]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵 (matrix) 是 array 的分⽀\n",
    "#dot : 矩阵乘法\n",
    "#对于多维数组，数组 a 的最 后⼀维上的所有元素与数组 b 的倒数第⼆位上的所有元素的乘积和：dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
    "import numpy as np\n",
    "\n",
    "# 定义两个简单的矩阵\n",
    "m1 = np.array([[1, 2], [5, 6]], dtype=np.float32)\n",
    "m2 = np.array([[3, 4], [7, 8]], dtype=np.float32)\n",
    "# m1 = m1.reshape(2,2)  # 矩阵1最后1个维度 == 矩阵2倒数第2个维度\n",
    "# m2 = m2.reshape(2,2)\n",
    "\n",
    "# 使用 np.dot 进行矩阵乘法\n",
    "result_dot = np.dot(m1, m2)\n",
    "\n",
    "# 使用 @ 运算符进行矩阵乘法\n",
    "result_at = m1 @ m2\n",
    "\n",
    "print(\"矩阵 1:\")\n",
    "print(m1)\n",
    "print(\"矩阵 2:\")\n",
    "print(m2)\n",
    "print(\"使用 np.dot 得到的矩阵乘法结果:\")\n",
    "print(result_dot)\n",
    "print(\"使用 @ 运算符得到的矩阵乘法结果:\")\n",
    "print(result_at)\n",
    "\n",
    "# 创建一个全零矩阵，用于存储手动推演的结果\n",
    "# 结果矩阵的行数等于 matrix1 的行数，列数等于 matrix2 的列数\n",
    "manual_result = np.zeros((m1.shape[0], m2.shape[1]), dtype=np.float32)\n",
    "\n",
    "# 外层循环：遍历 matrix1 的每一行\n",
    "# i 表示结果矩阵的行索引\n",
    "for i in range(m1.shape[0]):\n",
    "    # 中层循环：遍历 matrix2 的每一列\n",
    "    # j 表示结果矩阵的列索引\n",
    "    for j in range(m2.shape[1]):\n",
    "        # 初始化当前位置的结果为 0\n",
    "        manual_result[i, j] = 0\n",
    "        # 内层循环：计算 matrix1 的第 i 行与 matrix2 的第 j 列对应元素的乘积之和\n",
    "        # k 表示参与乘法运算的元素索引\n",
    "        for k in range(m1.shape[1]):\n",
    "            # 打印当前正在计算的元素\n",
    "            print(f\"{m1[i, k]} * {m2[k, j]} = {m1[i, k] * m2[k, j]}\")\n",
    "            # 将 matrix1 的第 i 行第 k 列元素与 matrix2 的第 k 行第 j 列元素相乘，并累加到结果矩阵的相应位置\n",
    "            manual_result[i, j] += m1[i, k] * m2[k, j]\n",
    "        # 打印当前位置计算完成后的结果\n",
    "        print(f\"结果矩阵[{i+1},{j+1}]:{manual_result[i, j]}\\n\")\n",
    "\n",
    "print(\"手动推演结果:\")\n",
    "print(manual_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#张量（Tensor）是pytorch中的基本单位。张量可以通过多种⽅式初始化\n",
    "#直接从数据\n",
    "data = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "print(f\"Ones Tensor: \\n {data} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/6lf5pyld0xb9xlts3t6lqhww0000gn/T/ipykernel_29193/306022106.py:2: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  np_array = np.array(data)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#从 NumPy 数组\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m np_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x_np \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np_array)\n\u001b[1;32m      4\u001b[0m x_np\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "#从 NumPy 数组\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7677, 0.3473],\n",
      "        [0.4959, 0.7623]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#从另⼀个张量\n",
    "#除⾮明确覆盖，否则新张量保留参数张量的属性（形状、数据类型）\n",
    "# 保留of x_data的属性\n",
    "x_ones = torch.ones_like(x_data) \n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "# 覆盖 x_data的数据类型\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) \n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[[0.2119, 0.6673, 0.8188, 0.5054, 0.7811, 0.6199],\n",
      "         [0.9841, 0.4903, 0.8465, 0.8162, 0.5432, 0.7596],\n",
      "         [0.6471, 0.6825, 0.2306, 0.5546, 0.7731, 0.5331]],\n",
      "\n",
      "        [[0.6502, 0.9746, 0.2041, 0.1561, 0.4660, 0.1474],\n",
      "         [0.9866, 0.3904, 0.1190, 0.0582, 0.8299, 0.4190],\n",
      "         [0.2904, 0.9580, 0.2341, 0.7628, 0.1571, 0.2873]]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "#使⽤随机值或常量值\n",
    "#shape 是张量维度的元组\n",
    "shape = (2,3,6)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape) \n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7])\n",
      "tensor([[0.8877, 0.5817, 0.8664],\n",
      "        [0.9873, 0.2952, 0.2323],\n",
      "        [0.4886, 0.9419, 0.3734],\n",
      "        [0.3754, 0.1695, 0.7181],\n",
      "        [0.8023, 0.9756, 0.1511]])\n",
      "tensor([[-0.1071, -1.1014, -0.3609],\n",
      "        [ 0.7813,  1.2428,  2.7649],\n",
      "        [ 0.1826,  0.1408, -2.3705],\n",
      "        [-0.0988,  1.6307, -0.8543],\n",
      "        [ 0.2179, -1.4866,  0.1841]])\n",
      "tensor([[ 0.2881,  0.4721, -0.6892],\n",
      "        [-0.3511, -2.1039, -0.1109],\n",
      "        [ 0.7220,  0.1826, -1.4409],\n",
      "        [-0.4893, -0.6112,  1.9838],\n",
      "        [ 0.5608,  0.0449, -0.7493]])\n",
      "tensor([ 1.0000,  1.4500,  1.9000,  2.3500,  2.8000,  3.2500,  3.7000,  4.1500,\n",
      "         4.6000,  5.0500,  5.5000,  5.9500,  6.4000,  6.8500,  7.3000,  7.7500,\n",
      "         8.2000,  8.6500,  9.1000,  9.5500, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "# 基于现有tensor构建，但使用新值填充\n",
    "m = torch.ones(5,7, dtype=torch.double)\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "\n",
    "# 获取tensor的大小\n",
    "print(m.size()) # torch.Size([5,3])\n",
    "\n",
    "# 均匀分布\n",
    "print(torch.rand(5,3))\n",
    "\n",
    "# 标准正态分布\n",
    "print(torch.randn(5,3))\n",
    "\n",
    "# 离散正态分布\n",
    "print(torch.normal(mean=.0,std=1.0,size=(5,3)))\n",
    "\n",
    "# 线性间隔向量(返回一个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "print(torch.linspace(start=1,end=10,steps=21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "#张量的属性\n",
    "#张量的属性描述了张量的形状、数据类型和存储它们的设备\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6427, 0.8492, 0.2450, 0.3289],\n",
      "        [0.4789, 0.8212, 0.2665, 0.3992],\n",
      "        [0.8878, 0.9396, 0.8116, 0.1932]], device='mps:0')\n",
      "mps:0\n",
      "tensor([[0.6427, 0.8492, 0.2450, 0.3289],\n",
      "        [0.4789, 0.8212, 0.2665, 0.3992],\n",
      "        [0.8878, 0.9396, 0.8116, 0.1932]], device='mps:0')\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "# 默认情况下，张量是在 CPU 上创建的。我们可以使⽤使⽤ .to() ⽅法明确地将张量移动 到 GPU\n",
    "# 设置张量在GPU上运算\n",
    "if torch.cuda.is_available():\n",
    " tensor = tensor.to('cuda')\n",
    "\n",
    "\n",
    "# 检查pytorch是否支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)\n",
    "\n",
    "\n",
    "# mac上没有GPU，使用M系列芯片\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#张量的索引和切⽚：\n",
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.],\n",
      "        [3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.],\n",
      "        [3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.],\n",
      "        [3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.]])\n",
      "torch.Size([4, 12])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n",
      "tensor([[[3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.]],\n",
      "\n",
      "        [[3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.]],\n",
      "\n",
      "        [[3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.]],\n",
      "\n",
      "        [[3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.],\n",
      "         [3., 0., 3., 3.]]])\n",
      "torch.Size([4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 张量的拼接\n",
    "# torch.cat\n",
    "# torch.stack\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "print(t1 * 3)\n",
    "print(t1.shape)\n",
    "\n",
    "\n",
    "t1 = torch.stack([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "print(t1 * 3)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n",
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.arange(1,10, dtype=torch.float32).reshape(3, 3)\n",
    "\n",
    "# 计算两个张量之间矩阵乘法的几种方式。 y1, y2, y3 最后的值是一样的 dot\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print(y3)\n",
    "\n",
    "\n",
    "# 计算张量逐元素相乘的几种方法。 z1, z2, z3 最后的值是一样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print(z1)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#可以使⽤ item() ⽅法将其转换为Python 数值\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70., 71., 72.],\n",
      "        [73., 74., 75.],\n",
      "        [76., 77., 78.]]) \n",
      "\n",
      "tensor([[80., 81., 82.],\n",
      "        [83., 84., 85.],\n",
      "        [86., 87., 88.]])\n"
     ]
    }
   ],
   "source": [
    "#In-place操作：把计算结果存储到当前操作数中的操作就称为就地操作\n",
    "#注：它会⽴即丢失历史记录\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)  #将改变 tensor ⾃⾝的值\n",
    "# tensor = tensor + 5\n",
    "# tensor += 5\n",
    "print(tensor+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pytorch计算图可视化\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 定义矩阵 A，向量 b 和常数 c\u001b[39;00m\n\u001b[1;32m      6\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# requires_grad=True 表示我们要对 A 求导\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "#pytorch计算图可视化\n",
    "#torchviz\n",
    "\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 定义矩阵 A，向量 b 和常数 c\n",
    "A = torch.randn(10, 10,requires_grad=True)  # requires_grad=True 表示我们要对 A 求导\n",
    "b = torch.randn(10,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "\n",
    "\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n",
    "\n",
    "# 生成计算图节点\n",
    "dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
